{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from coin_ai.alignment.data import HPairDataset, HomographyBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_corners(homography_batch: HomographyBatch):\n",
    "    assert homography_batch.B == 1\n",
    "\n",
    "    fig, (a1, a2) = plt.subplots(1, 2)\n",
    "    im1, im2 = homography_batch.images.squeeze(1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    a1.imshow(im1)\n",
    "    a2.imshow(im2)\n",
    "\n",
    "    corners_base = homography_batch.corners.squeeze(0).cpu().numpy()\n",
    "    corners_base = corners_base / 2 + 128\n",
    "    corners_base = np.concatenate([corners_base, np.ones((4, 1))], axis=1)\n",
    "    corners_warp = corners_base @ homography_batch.H_12.squeeze(0).cpu().numpy().T\n",
    "\n",
    "    corners_warp /= corners_warp[:, 2:]\n",
    "    a2.plot(corners_warp[:, 0][[0, 1, 2, 3, 0]], corners_warp[:, 1][[0, 1, 2, 3, 0]], 'r--')\n",
    "    a1.plot(corners_base[:, 0][[0, 1, 2, 3, 0]], corners_base[:, 1][[0, 1, 2, 3, 0]], 'r--')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "r = R.from_euler('xyz', [0, 0, 10], degrees=True)\n",
    "r.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "def augmentation(batch: HomographyBatch) -> HomographyBatch:\n",
    "    #scale = torch.eye(3)\n",
    "    #scale[0, 1] = 0.1\n",
    "    #scale[:2, :2] = torch.tensor(r.as_matrix(), dtype=torch.float32)\n",
    "    scale = torch.from_numpy(r.as_matrix()).to(torch.float32)\n",
    "    transform = torch.stack([\n",
    "        torch.eye(3),\n",
    "        scale,\n",
    "    ]).unsqueeze(1)\n",
    "\n",
    "    #return batch.build_augmentation().apply(batch.get_alignment_transform()).apply(transform).build()\n",
    "    return batch.build_augmentation().apply(transform).build()\n",
    "\n",
    "def no_aug(batch: HomographyBatch) -> HomographyBatch:\n",
    "    #return batch.build_augmentation().apply(batch.get_alignment_transform()).build()\n",
    "    return batch.build_augmentation().build()\n",
    "\n",
    "def assemble_datasets(path: str, augmentation: Callable[[HomographyBatch, ], HomographyBatch]) -> ConcatDataset:\n",
    "    paths = glob.glob(f\"{path}/**/homographies.csv\", recursive=True)\n",
    "\n",
    "    datasets = [HPairDataset(path, augmentation=augmentation, skip_identity=True, infer=False) for path in paths]\n",
    "    return ConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_aug = assemble_datasets('/Users/jatentaki/Data/archeo/coins/krzywousty-homographies', augmentation=augmentation)\n",
    "dataset_base = assemble_datasets('/Users/jatentaki/Data/archeo/coins/krzywousty-homographies', augmentation=no_aug)\n",
    "dataloader_aug = DataLoader(dataset_aug, batch_size=8, shuffle=False, collate_fn=HPairDataset.collate_fn)\n",
    "dataloader_base = DataLoader(dataset_base, batch_size=8, shuffle=False, collate_fn=HPairDataset.collate_fn)\n",
    "batch_aug = next(iter(dataloader_aug))\n",
    "batch_base = next(iter(dataloader_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(batch.B):\n",
    "#     fig = draw_corners(batch.slice(i, i + 1))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import kornia.geometry as KG\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from coin_ai.alignment.infra import DenseDino\n",
    "\n",
    "@dataclass\n",
    "class HCorrespondences:\n",
    "    corners_a: Tensor # B x 4 x 2\n",
    "    corners_b: Tensor # B x 4 x 2\n",
    "    \n",
    "    def implied_homography(self) -> Tensor:\n",
    "        return KG.get_perspective_transform(self.corners_a.cpu(), self.corners_b.cpu())\n",
    "    \n",
    "def compute_gt_corners(H_12: Tensor, corners: Tensor) -> Tensor:\n",
    "    corners_h = torch.cat([\n",
    "        corners,\n",
    "        torch.ones(H_12.shape[0], 4, 1, device=H_12.device),\n",
    "    ], dim=-1)\n",
    "\n",
    "    return (corners_h @ H_12.mT)[..., :2]\n",
    "\n",
    "def homography_loss(homography_batch: HomographyBatch, model: Callable[[Tensor, ], HCorrespondences]) -> Tensor:\n",
    "    predictions = model(homography_batch.images)\n",
    "    corners_b_gt = compute_gt_corners(homography_batch.H_12, predictions.corners_a)\n",
    "\n",
    "    return nn.functional.mse_loss(predictions.corners_b, corners_b_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryInit(nn.Module):\n",
    "    def __init__(self, d_memory: int, d_target: int):\n",
    "        super().__init__()\n",
    "        self.n_heads = 8\n",
    "        self.q = nn.Parameter(torch.randn(self.n_heads, 4, d_memory))\n",
    "        self.v = nn.Linear(d_memory, d_target)\n",
    "        self.o = nn.Linear(d_target, d_target)\n",
    "\n",
    "    def forward(self, src_features: Tensor) -> Tensor:\n",
    "        b, n, c = src_features.shape\n",
    "        q = repeat(self.q, 'h q c -> b h q c', b=b)\n",
    "        k = rearrange(src_features, 'b n c -> b 1 n c') # full attention\n",
    "        v = rearrange(self.v(src_features), 'b n (h c) -> b h n c', h=self.n_heads)\n",
    "        queries = nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "        return self.o(rearrange(queries, 'b h n c -> b n (h c)'))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_io: int, d_ff: int | None = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_io\n",
    "\n",
    "        self.fc1 = nn.Linear(d_io, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_io)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.fc2(nn.functional.gelu(self.fc1(x)))\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_src: int, d_tgt: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.q = nn.Linear(d_tgt, d_tgt)\n",
    "        self.kv = nn.Linear(d_src, 2 * d_tgt)\n",
    "        self.o = nn.Linear(d_tgt, d_tgt)\n",
    "    \n",
    "    def forward(self, target: Tensor, memory: Tensor) -> Tensor:\n",
    "        b, q, c = target.shape\n",
    "\n",
    "        q = rearrange(self.q(target), 'b q (h c) -> b h q c', h=self.n_heads)\n",
    "        k, v = rearrange(self.kv(memory), 'b n (t h c) -> t b h n c', h=self.n_heads, t=2)\n",
    "\n",
    "        out = nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "        return self.o(rearrange(out, 'b h n c -> b n (h c)'))\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_memory: int, d_target: int, n_heads: int, d_ff: int | None = None):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_target)\n",
    "        self.cross_attention = CrossAttention(d_src=d_memory, d_tgt=d_target, n_heads=n_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_target)\n",
    "        self.mlp = MLP(d_target, d_ff)\n",
    "    \n",
    "    def forward(self, queries: Tensor, memory: Tensor) -> Tensor:\n",
    "        q = queries\n",
    "        q = q + self.cross_attention(self.norm1(q), memory)\n",
    "        q = q + self.mlp(self.norm2(q))\n",
    "        return q\n",
    "\n",
    "class HFormer(nn.Module):\n",
    "    def __init__(self, n_layers: int = 3, d_target: int = 128, deformation_scale: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dino = DenseDino()\n",
    "        self.dino.requires_grad_(False)\n",
    "        d_memory = 384\n",
    "        self.n_heads = 8\n",
    "        self.query_init = QueryInit(d_memory=d_memory, d_target=d_target)\n",
    "        self.attn_blocks = nn.ModuleList([\n",
    "            CrossAttentionBlock(d_memory=d_memory, d_target=d_target, n_heads=self.n_heads)\n",
    "            for _ in range(2 * n_layers - 1)\n",
    "        ])\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(d_target)\n",
    "        self.xy_head = nn.Linear(d_target, 2, bias=False)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'corners_a',\n",
    "            torch.tensor([\n",
    "                [0., 0.],\n",
    "                [1., 0.],\n",
    "                [1., 1.],\n",
    "                [0., 1.]\n",
    "            ], dtype=torch.float32).reshape(1, 4, 2) * 0.5 + 0.25,\n",
    "        )\n",
    "        self.deformation_scale = deformation_scale\n",
    "    \n",
    "    def forward(self, images: Tensor) -> HCorrespondences:\n",
    "        b = images.shape[1]\n",
    "\n",
    "        src_feat, dst_feat = self._get_features(images)\n",
    "        q = self.query_init(src_feat)\n",
    "\n",
    "        for i, block in enumerate(self.attn_blocks):\n",
    "            if i % 2 == 0:\n",
    "                memory = dst_feat\n",
    "            else:\n",
    "                memory = src_feat\n",
    "\n",
    "            q = block(q, memory)\n",
    "        \n",
    "        offset = torch.tanh(self.xy_head(self.final_norm(q)))\n",
    "\n",
    "        corners_b = self.corners_a + self.deformation_scale * offset\n",
    "\n",
    "        scale = torch.tensor([images.shape[-1], images.shape[-2]], dtype=torch.float32, device=images.device)\n",
    "        return HCorrespondences(\n",
    "            corners_a=self.corners_a.repeat(b, 1, 1) * scale,\n",
    "            corners_b=corners_b * scale,\n",
    "        )\n",
    "    \n",
    "    def _get_features(self, images: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Images: 2 x B x C x H x W\n",
    "        \n",
    "        Returns:\n",
    "            src_feat: B x N x C\n",
    "            dst_feat: B x N x C\n",
    "        \"\"\"\n",
    "        t, b, c, h, w = images.shape\n",
    "        assert t == 2\n",
    "        assert c == 3\n",
    "\n",
    "        images_flat = rearrange(images, 't b c h w -> (t b) c h w')\n",
    "        with torch.no_grad():\n",
    "            features_flat: Tensor = self.dino(images_flat)\n",
    "        src_feat, dst_feat = rearrange(features_flat, '(t b) h w c -> t b (h w) c', b=b, t=2)\n",
    "        return src_feat, dst_feat\n",
    "\n",
    "    def loss_fn(self, homography_batch: HomographyBatch) -> Tensor:\n",
    "        return homography_loss(homography_batch, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "hformer = HFormer().to(device)\n",
    "optim = torch.optim.AdamW(hformer.parameters(), lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = sum(p.numel() for p in hformer.parameters() if p.requires_grad)\n",
    "# print(f\"{n:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# for _ in range(1):\n",
    "#     losses = []\n",
    "#     for batch in tqdm(dataloader):\n",
    "#         optim.zero_grad()\n",
    "#         batch = batch.to(device)\n",
    "#         loss = hformer.loss_fn(batch)\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "#         losses.append(loss.item())\n",
    "#     print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_aug = hformer(batch_aug.images.to(device))\n",
    "    prediction_base = hformer(batch_base.images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(batch, prediction):\n",
    "    corners_a = prediction.corners_a.cpu()\n",
    "    corners_b = prediction.corners_b.cpu()\n",
    "    corners_b_gt = compute_gt_corners(batch.H_12.to('cpu'), corners_a)\n",
    "\n",
    "    rep = [0, 1, 2, 3, 0]\n",
    "    for s in range(batch_aug.B):\n",
    "        fig, (a1, a2) = plt.subplots(1, 2)\n",
    "        a1.imshow(batch.images[0, s].permute(1, 2, 0).cpu().numpy())\n",
    "        a2.imshow(batch.images[1, s].permute(1, 2, 0).cpu().numpy())\n",
    "        a1.plot(corners_a[s, :, 1][rep], corners_a[s, :, 0][rep], 'r--')\n",
    "    #    a2.scatter(corners_b[s, :, 0], corners_b[s, :, 1], color='r')\n",
    "        a2.plot(corners_b_gt[s, :, 1][rep], corners_b_gt[s, :, 0][rep], 'r--')\n",
    "    \n",
    "plot(batch_aug, prediction_aug)\n",
    "plt.show()\n",
    "print('-' * 80)\n",
    "plot(batch_base, prediction_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_aug.corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia.geometry as KG\n",
    "\n",
    "unwarped_batch = batch_aug.to('cpu')\n",
    "s = (3, 4)\n",
    "for _ in range(5):\n",
    "    fig = draw_corners(unwarped_batch.slice(*s))\n",
    "    with torch.no_grad():\n",
    "        prediction_aug = hformer(unwarped_batch.images.to(device))\n",
    "\n",
    "    transform_12 = KG.get_perspective_transform(prediction_aug.corners_a.repeat(8, 1, 1).cpu() * 518, prediction_aug.corners_b.cpu() * 518)\n",
    "    transform = torch.stack([\n",
    "        torch.eye(3).reshape(1, 3, 3).repeat(8, 1, 1),\n",
    "        torch.linalg.inv(transform_12),\n",
    "    ], dim=0)\n",
    "\n",
    "    unwarped_batch = unwarped_batch.build_augmentation().apply(transform).to('cpu').build()\n",
    "\n",
    "fig = draw_corners(unwarped_batch.slice(*s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_aug.corners_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pred = batch_aug.build_augmentation().apply(transform).to('cpu').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_pred.B):\n",
    "    fig1 = draw_corners(batch_aug.slice(i, i + 1))\n",
    "    fig2 = draw_corners(batch_pred.slice(i, i + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
