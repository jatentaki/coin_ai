{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "import csv\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from coin_ai.fg_segmentation import SegmentationDino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Homography:\n",
    "    path1: str\n",
    "    path2: str\n",
    "    H: np.ndarray\n",
    "\n",
    "    @property\n",
    "    def image1(self) -> np.ndarray:\n",
    "        return imageio.imread(self.path1)\n",
    "    \n",
    "    @property\n",
    "    def image2(self) -> np.ndarray:\n",
    "        return imageio.imread(self.path2)\n",
    "    \n",
    "    def inverse(self) -> Homography:\n",
    "        return Homography(self.path2, self.path1, np.linalg.inv(self.H))\n",
    "\n",
    "\n",
    "def parse_homography_csv(source_path: str) -> list[Homography]:\n",
    "    HEADERS = ['img1', 'h11', 'h12', 'h13', 'h21', 'h22', 'h23', 'h31', 'h32', 'h33']\n",
    "\n",
    "    if not os.path.exists(source_path):\n",
    "        raise ValueError(f'File {source_path} does not exist')\n",
    "\n",
    "    pairs = []\n",
    "    with open(source_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader)\n",
    "        assert headers[:len(HEADERS)] == HEADERS\n",
    "\n",
    "        for img1, img2, *floats in reader:\n",
    "            H_floats = floats[:9]\n",
    "            source_dir = os.path.dirname(source_path)\n",
    "            path1 = os.path.join(source_dir, img1)\n",
    "            path2 = os.path.join(source_dir, img2)\n",
    "            H = np.array(H_floats, dtype=np.float32).reshape(3, 3)\n",
    "            H = H[[1, 0, 2]][:, [1, 0, 2]] # undo the xy flip\n",
    "            H = np.linalg.inv(H)\n",
    "            pairs.append(Homography(\n",
    "                path1, path2, H,\n",
    "            ))\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, Tensor\n",
    "from einops import rearrange\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from kornia.utils import image_to_tensor\n",
    "\n",
    "\n",
    "class DenseDino(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dino = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14_reg\")\n",
    "        self.dino.eval()\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x: Tensor | np.ndarray) -> Tensor:\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = image_to_tensor(x, keepdim=False)\n",
    "\n",
    "        b, c, h, w = x.shape\n",
    "        assert (c, h, w) == (3, 518, 518)\n",
    "\n",
    "        if x.dtype == torch.uint8:\n",
    "            x = transforms.ToDtype(torch.float32, scale=True)(x)\n",
    "        x = transforms.Grayscale(num_output_channels=3)(x)\n",
    "        x = x.to(self.device)\n",
    "        x = self.dino.forward_features(x)[\"x_norm_patchtokens\"]\n",
    "        return rearrange(x, \"b (h w) c -> b h w c\", h=37, w=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from kornia.feature import match_mnn, match_smnn\n",
    "\n",
    "@dataclass\n",
    "class MatchingPatches:\n",
    "    left: Tensor # (h, w, c)\n",
    "    right: Tensor # (h, w, c)\n",
    "    is_correct_match: Tensor # (h, w, h, w)\n",
    "\n",
    "    def to(self, device: torch.device) -> MatchingPatches:\n",
    "        return MatchingPatches(\n",
    "            left=self.left.to(device),\n",
    "            right=self.right.to(device),\n",
    "            is_correct_match=self.is_correct_match.to(device),\n",
    "        )\n",
    "    \n",
    "    def flip(self) -> MatchingPatches:\n",
    "        return MatchingPatches(\n",
    "            left=self.right,\n",
    "            right=self.left,\n",
    "            is_correct_match=rearrange(self.is_correct_match, 'i j k l -> k l i j'),\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        shape = lambda t: tuple(t.shape)\n",
    "        return f'MatchingPatches(left={shape(self.left)}, right={shape(self.right)}, is_correct_match={shape(self.is_correct_match)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_patches(dense_dino: DenseDino, segmentation: nn.Linear, image1: np.ndarray, image2: np.ndarray, H: np.ndarray, threshold: float = 1.4) -> MatchingPatches:\n",
    "    H = torch.from_numpy(H).clone()\n",
    "    H[:2, -1] /= 14\n",
    "\n",
    "    with torch.no_grad():\n",
    "        left_feat = dense_dino(image1).cpu().squeeze(0)\n",
    "        right_feat = dense_dino(image2).cpu().squeeze(0)\n",
    "\n",
    "        left_fg_mask = segmentation(left_feat).cpu().squeeze(0).squeeze(-1) > 0\n",
    "        right_fg_mask = segmentation(right_feat).cpu().squeeze(0).squeeze(-1) > 0\n",
    "\n",
    "    grid = torch.stack(torch.meshgrid(\n",
    "        torch.arange(37),\n",
    "        torch.arange(37),\n",
    "        indexing='xy',\n",
    "    ), dim=-1).to(torch.float32) + 0.5\n",
    "\n",
    "    grid_homo = torch.cat([\n",
    "        grid,\n",
    "        torch.ones((37, 37, 1), device=grid.device, dtype=grid.dtype),\n",
    "    ], dim=-1)\n",
    "\n",
    "    left_mapped = torch.einsum('ij,hwj->hwi', H, grid_homo)\n",
    "    left_mapped = left_mapped[..., 0:2] / left_mapped[..., 2:3]\n",
    "\n",
    "    distances = torch.linalg.norm(\n",
    "        left_mapped[:, :, None, None, :]- grid[None, None, :, :, :],\n",
    "        dim=-1,\n",
    "    )\n",
    "\n",
    "    is_correct_match = (distances < threshold) & left_fg_mask[..., None, None] & right_fg_mask[None, None, ...]\n",
    "\n",
    "    return MatchingPatches(\n",
    "        left_feat,\n",
    "        right_feat,\n",
    "        is_correct_match,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointExtractor(nn.Module):\n",
    "    def __init__(self, dino: DenseDino, segmenter: nn.Linear, descriptor: nn.Linear):\n",
    "        super().__init__()\n",
    "        self.dino = dino\n",
    "        self.segmenter = segmenter\n",
    "        self.descriptor = descriptor\n",
    "\n",
    "    @torch.no_grad\n",
    "    def forward(self, image: np.ndarray) -> tuple[Tensor, Tensor]:\n",
    "        raw_features = self.dino(image).squeeze(0)\n",
    "        is_foreground = self.segmenter(raw_features).squeeze(-1) > 0\n",
    "        matching_features = self.descriptor(raw_features)\n",
    "        matching_features = F.normalize(matching_features, dim=-1)\n",
    "\n",
    "        coord_grid = torch.stack(\n",
    "            torch.meshgrid(\n",
    "                torch.arange(37, device=raw_features.device),\n",
    "                torch.arange(37, device=raw_features.device),\n",
    "                indexing='xy',\n",
    "            ),\n",
    "            dim=-1,\n",
    "        ).to(torch.float32) * 14 + 7\n",
    "\n",
    "        return coord_grid[is_foreground], matching_features[is_foreground]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure, Axes\n",
    "\n",
    "def get_matching_keypoints(kp1: Tensor, kp2: Tensor, idxs: Tensor) -> Tensor:\n",
    "    mkpts1 = kp1[idxs[:, 0]]\n",
    "    mkpts2 = kp2[idxs[:, 1]]\n",
    "    return mkpts1, mkpts2\n",
    "\n",
    "class ImageAligner:\n",
    "    def __init__(self, extractor: KeypointExtractor, n_steps: int = 3, with_visualization: bool = True):\n",
    "        self.extractor = extractor\n",
    "        self.n_steps = n_steps\n",
    "        self.with_visualization = with_visualization\n",
    "    \n",
    "    def find_matches(self, image1: np.ndarray, image2: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        kps1, descs1 = self.extractor(image1)\n",
    "        kps2, descs2 = self.extractor(image2)\n",
    "\n",
    "        _dists, idxs = match_smnn(descs1, descs2, 0.90)\n",
    "        mkpts1, mkpts2 = get_matching_keypoints(kps1, kps2, idxs)\n",
    "\n",
    "        return mkpts1.cpu().numpy(), mkpts2.cpu().numpy()\n",
    "    \n",
    "    def find_homography(self, kpts1: np.ndarray, kpts2: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        Hm, inliers = cv2.findHomography(\n",
    "            srcPoints=kpts1,\n",
    "            dstPoints=kpts2,\n",
    "            method=cv2.RANSAC,\n",
    "            ransacReprojThreshold=14.,\n",
    "            confidence=0.999,\n",
    "            maxIters=100000,\n",
    "        )\n",
    "\n",
    "        return Hm, inliers\n",
    "    \n",
    "    def warp_perpective(self, image: np.ndarray, H: np.ndarray) -> np.ndarray:\n",
    "        image_f32 = image.astype(np.float32) / 255\n",
    "        warp_f32 = cv2.warpPerspective(\n",
    "            image_f32,\n",
    "            np.linalg.inv(H),\n",
    "            (image.shape[1], image.shape[0]),\n",
    "            borderMode=cv2.BORDER_REPLICATE,\n",
    "        )\n",
    "\n",
    "        return (warp_f32 * 255).astype(np.uint8)\n",
    "    \n",
    "    def visualize_matches(\n",
    "        self,\n",
    "        image1: np.ndarray,\n",
    "        image2: np.ndarray,\n",
    "        mkpts1: np.ndarray,\n",
    "        mkpts2: np.ndarray,\n",
    "        inliers: np.ndarray,\n",
    "    ) -> tuple[Figure, Axes]:\n",
    "        kpts_to_cv2 = lambda kpts: [cv2.KeyPoint(x, y, 14) for x, y in kpts]\n",
    "\n",
    "        outlier_matches = [cv2.DMatch(i, i, 0) for i in range(len(mkpts1)) if not inliers[i]]\n",
    "        inlier_matches = [cv2.DMatch(i, i, 0) for i in range(len(mkpts1)) if inliers[i]]\n",
    "\n",
    "        image = cv2.drawMatches(\n",
    "            image1,\n",
    "            kpts_to_cv2(mkpts1),\n",
    "            image2,\n",
    "            kpts_to_cv2(mkpts2),\n",
    "            outlier_matches,\n",
    "            outImg=None,\n",
    "            matchColor=(0, 0, 255),\n",
    "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
    "        )\n",
    "\n",
    "        image = cv2.drawMatches(\n",
    "            image1,\n",
    "            kpts_to_cv2(mkpts1),\n",
    "            image2,\n",
    "            kpts_to_cv2(mkpts2),\n",
    "            inlier_matches,\n",
    "            outImg=image,\n",
    "            matchColor=(0, 255, 0),\n",
    "            flags=cv2.DrawMatchesFlags_DEFAULT,\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        return fig, ax\n",
    "\n",
    "    \n",
    "    def step(self, image1: np.ndarray, image2: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        mkpts1, mkpts2 = self.find_matches(image1, image2)\n",
    "\n",
    "        Hm, inliers = self.find_homography(mkpts1, mkpts2)\n",
    "\n",
    "        if self.with_visualization:\n",
    "            self.visualize_matches(image1, image2, mkpts1, mkpts2, inliers.squeeze(-1))\n",
    "            plt.show()\n",
    "\n",
    "        image1_warped = self.warp_perpective(image1, Hm)\n",
    "\n",
    "        return image1_warped, Hm\n",
    "\n",
    "    def __call__(self, image1: np.ndarray, image2: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        for _ in range(self.n_steps):\n",
    "            image1, _ = self.step(image1, image2)\n",
    "            plt.figure()\n",
    "            plt.imshow(image1)\n",
    "            plt.show()\n",
    "        return image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for dir, _, files in os.walk('/Users/jatentaki/Data/archeo/coins/cropped-with-background/Krzywousty/'):\n",
    "    for file in files:\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "        path = os.path.join(dir, file)\n",
    "        pairs.extend(parse_homography_csv(path))\n",
    "\n",
    "print(len(pairs))\n",
    "#pairs = parse_homography_csv('/Users/jatentaki/Data/archeo/coins/cropped-with-background/Krzywousty/Typ 3/Awers - stempel a.07/homographies.csv')\n",
    "main_pair = pairs[0]\n",
    "image1 = main_pair.image1\n",
    "image2 = main_pair.image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SegmentationDino()\n",
    "dense_dino = DenseDino()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = [match_patches(dense_dino, sd.head, image1, image2, pair.H, threshold=1.0) for pair in pairs]\n",
    "main_mp = mps[0]\n",
    "plt.imshow(main_mp.is_correct_match.cpu().numpy()[25, 15])\n",
    "plt.scatter([15], [25], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss_fn(mp: MatchingPatches, embed_fn: nn.Module, margin: float = 0.5) -> tuple[Tensor, dict[str, Tensor]]:\n",
    "    left = embed_fn(mp.left)\n",
    "    right = embed_fn(mp.right)\n",
    "\n",
    "    left = rearrange(left, \"h w c -> (h w) c\")\n",
    "    right = rearrange(right, \"h w c -> (h w) c\")\n",
    "    is_correct_match = rearrange(mp.is_correct_match, \"i j k l -> (i j) (k l)\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #_dists, idxs = match_mnn(left, right)\n",
    "        _dists, idxs = match_smnn(left, right, 0.8)\n",
    "    is_correct_choice = is_correct_match[idxs[..., 0], idxs[..., 1]]\n",
    "\n",
    "    is_any_correct = is_correct_match.any(dim=1)\n",
    "    left = left[is_any_correct]\n",
    "    is_correct_match = is_correct_match[is_any_correct]\n",
    "\n",
    "    similarity = F.cosine_similarity(left[:, None, :], right[None, :], dim=-1)\n",
    "\n",
    "    negative_similarity = torch.where(\n",
    "        ~is_correct_match,\n",
    "        similarity,\n",
    "        torch.tensor(0.0, device=similarity.device, dtype=similarity.dtype),\n",
    "    )\n",
    "\n",
    "    positive_similarity = torch.where(\n",
    "        is_correct_match,\n",
    "        similarity,\n",
    "        torch.tensor(-1.0, device=similarity.device, dtype=similarity.dtype),\n",
    "    ).max(dim=1, keepdim=True).values\n",
    "\n",
    "    loss = F.relu(negative_similarity - positive_similarity + margin)\n",
    "    n_nonzero = (loss > 0).sum()\n",
    "    loss = loss.sum() / n_nonzero\n",
    "    \n",
    "    accuracy = is_correct_choice.float().mean().item() if is_correct_choice.numel() else 0.0\n",
    "    metrics = {\n",
    "        'loss': loss.item(),\n",
    "        'accuracy': accuracy,\n",
    "        'n_matches': is_correct_choice.numel(),\n",
    "    }\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "def loss_fn(mp: MatchingPatches, embed_fn: nn.Module, margin: float = 0.5) -> tuple[Tensor, dict[str, Tensor]]:\n",
    "    left = embed_fn(mp.left)\n",
    "    right = embed_fn(mp.right)\n",
    "\n",
    "    left = rearrange(left, \"h w c -> (h w) c\")\n",
    "    right = rearrange(right, \"h w c -> (h w) c\")\n",
    "    similarity = F.cosine_similarity(left[:, None, :], right[None, :], dim=-1)\n",
    "    is_correct_match = rearrange(mp.is_correct_match, \"i j k l -> (i j) (k l)\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _dists, idxs = match_smnn(left, right, dm=1 - similarity, th=0.8)\n",
    "    is_correct_choice = is_correct_match[idxs[..., 0], idxs[..., 1]]\n",
    "\n",
    "    loss_lr, n_lr = _one_way(similarity, is_correct_match, margin=margin)\n",
    "    loss_rl, n_rl = _one_way(similarity.T, is_correct_match.T, margin=margin)\n",
    "\n",
    "    loss = (loss_lr + loss_rl) / (n_lr + n_rl)\n",
    "\n",
    "    accuracy = is_correct_choice.float().mean().item() if is_correct_choice.numel() else 0.0\n",
    "    metrics = {\n",
    "        'loss': loss.item(),\n",
    "        'accuracy': accuracy,\n",
    "        'n_matches': is_correct_choice.numel(),\n",
    "    }\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "def _one_way(similarity: Tensor, is_correct_match: Tensor, margin: float = 0.5) -> tuple[Tensor, Tensor]:\n",
    "    is_any_correct = is_correct_match.any(dim=1)\n",
    "    similarity = similarity[is_any_correct]\n",
    "    is_correct_match = is_correct_match[is_any_correct]\n",
    "\n",
    "    negative_similarity = torch.where(\n",
    "        ~is_correct_match,\n",
    "        similarity,\n",
    "        torch.tensor(0.0, device=similarity.device, dtype=similarity.dtype),\n",
    "    )\n",
    "\n",
    "    positive_similarity = torch.where(\n",
    "        is_correct_match,\n",
    "        similarity,\n",
    "        torch.tensor(-1.0, device=similarity.device, dtype=similarity.dtype),\n",
    "    ).max(dim=1, keepdim=True).values\n",
    "\n",
    "    loss = F.relu(negative_similarity - positive_similarity + margin)\n",
    "    n_nonzero = (loss > 0).sum()\n",
    "\n",
    "    return loss.sum(), n_nonzero\n",
    "\n",
    "def total_loss(mps: list[MatchingPatches], embedder: nn.Module, **kwargs) -> tuple[Tensor, dict[str, float]]:\n",
    "    total_loss = 0\n",
    "    total_metrics = {}\n",
    "    for mp in mps:\n",
    "        loss, metrics = loss_fn(mp, embedder, **kwargs)\n",
    "        total_loss = total_loss + loss\n",
    "        for k, v in metrics.items():\n",
    "            total_metrics[k] = total_metrics.get(k, 0) + v\n",
    "    \n",
    "    for k in total_metrics:\n",
    "        total_metrics[k] /= len(mps)\n",
    "\n",
    "    return total_loss, total_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('mps')\n",
    "embedder = nn.Sequential(\n",
    "    nn.LayerNorm(384, elementwise_affine=False, bias=False),\n",
    "    nn.Linear(384, 32),\n",
    ").to(device)\n",
    "mps_train = []\n",
    "for mp in mps[:2]:\n",
    "    mps_train.append(mp.to(device))\n",
    "optim = torch.optim.Adam(embedder.parameters(), lr=1e-3)\n",
    "#optim = torch.optim.SGD(embedder.parameters(), lr=1e-4)\n",
    "\n",
    "metrics_log = {}\n",
    "with tqdm(range(500)) as pbar:\n",
    "    for i in pbar:\n",
    "        random.shuffle(mps_train)\n",
    "        loss, metrics = total_loss(mps_train[:4], embedder, margin=0.25)\n",
    "        #loss, metrics = mp.loss(embedder, margin=0.25)\n",
    "        for k, v in metrics.items():\n",
    "            metrics_log.setdefault(k, []).append(v)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pbar.set_postfix(**metrics)\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "loss_ax.plot(metrics_log['loss'])\n",
    "loss_ax.set_ylabel('Loss')\n",
    "acc_ax = loss_ax.twinx()\n",
    "acc_ax.plot(metrics_log['accuracy'], color='orange')\n",
    "acc_ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_mp = main_mp.to(device)\n",
    "def show_similarity(point: tuple[int, int]) -> None:\n",
    "    fig, (a1, a2, a3, a4) = plt.subplots(1, 4, figsize=(20, 5), constrained_layout=True)\n",
    "    a1.imshow(image1)\n",
    "    a1.scatter([point[0] * 14], [point[1] * 14], c='r')\n",
    "    a2.imshow(image2)\n",
    "    similarity_base = F.cosine_similarity(main_mp.left.squeeze(0)[point[1], point[0]], main_mp.right.squeeze(0).flatten(0, -2), dim=-1)\n",
    "    argmax_x, argmax_y = np.unravel_index(similarity_base.argmax().cpu().numpy(), (37, 37))\n",
    "    a3.imshow(similarity_base.cpu().numpy().reshape(37, 37), vmin=-1, vmax=1)\n",
    "    a3.scatter([argmax_y], [argmax_x], c='r')\n",
    "    with torch.no_grad():\n",
    "        similarity_embedder = F.cosine_similarity(embedder(main_mp.left.squeeze(0))[point[1], point[0]], embedder(main_mp.right.squeeze(0).flatten(0, -2)), dim=-1)\n",
    "    similarity_embedder = similarity_embedder.cpu().numpy().reshape(37, 37)\n",
    "    argmax_x, argmax_y = np.unravel_index(similarity_embedder.argmax(), similarity_embedder.shape)\n",
    "    a4.imshow(similarity_embedder, vmin=-1, vmax=1)\n",
    "    a4.scatter([argmax_y], [argmax_x], c='r')\n",
    "    a2.scatter([argmax_y * 14], [argmax_x * 14], c='r')\n",
    "\n",
    "    for ax in (a1, a2, a3, a4):\n",
    "        ax.axis('off')\n",
    "\n",
    "show_similarity((25, 15))\n",
    "show_similarity((10, 13))\n",
    "show_similarity((10, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpe = KeypointExtractor(dense_dino, sd.head, embedder).to(device)\n",
    "aligner = ImageAligner(kpe, n_steps=3, with_visualization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = ImageAligner(kpe, 3)\n",
    "\n",
    "warp_1, H = aligner.step(image1, image2)\n",
    "plt.imsave('0_src.png', image1)\n",
    "plt.imsave('1_warp.png', warp_1)\n",
    "plt.imsave('2_target.png', image2)\n",
    "\n",
    "\n",
    "warp_1, warp_2 = aligner(image1, image2)\n",
    "\n",
    "plt.imshow(warp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
