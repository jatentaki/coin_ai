{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import io\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "from model import DinoWithHead, AttentionReadout\n",
    "from inference import InferenceImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/jatentaki/Data/archeo/coins/FMP/slices-high-res/just_coins'\n",
    "dataset = InferenceImageDataset(root)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jatentaki/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['dino.cls_token', 'dino.pos_embed', 'dino.register_tokens', 'dino.mask_token', 'dino.patch_embed.proj.weight', 'dino.patch_embed.proj.bias', 'dino.blocks.0.norm1.weight', 'dino.blocks.0.norm1.bias', 'dino.blocks.0.attn.qkv.weight', 'dino.blocks.0.attn.qkv.bias', 'dino.blocks.0.attn.proj.weight', 'dino.blocks.0.attn.proj.bias', 'dino.blocks.0.ls1.gamma', 'dino.blocks.0.norm2.weight', 'dino.blocks.0.norm2.bias', 'dino.blocks.0.mlp.fc1.weight', 'dino.blocks.0.mlp.fc1.bias', 'dino.blocks.0.mlp.fc2.weight', 'dino.blocks.0.mlp.fc2.bias', 'dino.blocks.0.ls2.gamma', 'dino.blocks.1.norm1.weight', 'dino.blocks.1.norm1.bias', 'dino.blocks.1.attn.qkv.weight', 'dino.blocks.1.attn.qkv.bias', 'dino.blocks.1.attn.proj.weight', 'dino.blocks.1.attn.proj.bias', 'dino.blocks.1.ls1.gamma', 'dino.blocks.1.norm2.weight', 'dino.blocks.1.norm2.bias', 'dino.blocks.1.mlp.fc1.weight', 'dino.blocks.1.mlp.fc1.bias', 'dino.blocks.1.mlp.fc2.weight', 'dino.blocks.1.mlp.fc2.bias', 'dino.blocks.1.ls2.gamma', 'dino.blocks.2.norm1.weight', 'dino.blocks.2.norm1.bias', 'dino.blocks.2.attn.qkv.weight', 'dino.blocks.2.attn.qkv.bias', 'dino.blocks.2.attn.proj.weight', 'dino.blocks.2.attn.proj.bias', 'dino.blocks.2.ls1.gamma', 'dino.blocks.2.norm2.weight', 'dino.blocks.2.norm2.bias', 'dino.blocks.2.mlp.fc1.weight', 'dino.blocks.2.mlp.fc1.bias', 'dino.blocks.2.mlp.fc2.weight', 'dino.blocks.2.mlp.fc2.bias', 'dino.blocks.2.ls2.gamma', 'dino.blocks.3.norm1.weight', 'dino.blocks.3.norm1.bias', 'dino.blocks.3.attn.qkv.weight', 'dino.blocks.3.attn.qkv.bias', 'dino.blocks.3.attn.proj.weight', 'dino.blocks.3.attn.proj.bias', 'dino.blocks.3.ls1.gamma', 'dino.blocks.3.norm2.weight', 'dino.blocks.3.norm2.bias', 'dino.blocks.3.mlp.fc1.weight', 'dino.blocks.3.mlp.fc1.bias', 'dino.blocks.3.mlp.fc2.weight', 'dino.blocks.3.mlp.fc2.bias', 'dino.blocks.3.ls2.gamma', 'dino.blocks.4.norm1.weight', 'dino.blocks.4.norm1.bias', 'dino.blocks.4.attn.qkv.weight', 'dino.blocks.4.attn.qkv.bias', 'dino.blocks.4.attn.proj.weight', 'dino.blocks.4.attn.proj.bias', 'dino.blocks.4.ls1.gamma', 'dino.blocks.4.norm2.weight', 'dino.blocks.4.norm2.bias', 'dino.blocks.4.mlp.fc1.weight', 'dino.blocks.4.mlp.fc1.bias', 'dino.blocks.4.mlp.fc2.weight', 'dino.blocks.4.mlp.fc2.bias', 'dino.blocks.4.ls2.gamma', 'dino.blocks.5.norm1.weight', 'dino.blocks.5.norm1.bias', 'dino.blocks.5.attn.qkv.weight', 'dino.blocks.5.attn.qkv.bias', 'dino.blocks.5.attn.proj.weight', 'dino.blocks.5.attn.proj.bias', 'dino.blocks.5.ls1.gamma', 'dino.blocks.5.norm2.weight', 'dino.blocks.5.norm2.bias', 'dino.blocks.5.mlp.fc1.weight', 'dino.blocks.5.mlp.fc1.bias', 'dino.blocks.5.mlp.fc2.weight', 'dino.blocks.5.mlp.fc2.bias', 'dino.blocks.5.ls2.gamma', 'dino.blocks.6.norm1.weight', 'dino.blocks.6.norm1.bias', 'dino.blocks.6.attn.qkv.weight', 'dino.blocks.6.attn.qkv.bias', 'dino.blocks.6.attn.proj.weight', 'dino.blocks.6.attn.proj.bias', 'dino.blocks.6.ls1.gamma', 'dino.blocks.6.norm2.weight', 'dino.blocks.6.norm2.bias', 'dino.blocks.6.mlp.fc1.weight', 'dino.blocks.6.mlp.fc1.bias', 'dino.blocks.6.mlp.fc2.weight', 'dino.blocks.6.mlp.fc2.bias', 'dino.blocks.6.ls2.gamma', 'dino.blocks.7.norm1.weight', 'dino.blocks.7.norm1.bias', 'dino.blocks.7.attn.qkv.weight', 'dino.blocks.7.attn.qkv.bias', 'dino.blocks.7.attn.proj.weight', 'dino.blocks.7.attn.proj.bias', 'dino.blocks.7.ls1.gamma', 'dino.blocks.7.norm2.weight', 'dino.blocks.7.norm2.bias', 'dino.blocks.7.mlp.fc1.weight', 'dino.blocks.7.mlp.fc1.bias', 'dino.blocks.7.mlp.fc2.weight', 'dino.blocks.7.mlp.fc2.bias', 'dino.blocks.7.ls2.gamma', 'dino.blocks.8.norm1.weight', 'dino.blocks.8.norm1.bias', 'dino.blocks.8.attn.qkv.weight', 'dino.blocks.8.attn.qkv.bias', 'dino.blocks.8.attn.proj.weight', 'dino.blocks.8.attn.proj.bias', 'dino.blocks.8.ls1.gamma', 'dino.blocks.8.norm2.weight', 'dino.blocks.8.norm2.bias', 'dino.blocks.8.mlp.fc1.weight', 'dino.blocks.8.mlp.fc1.bias', 'dino.blocks.8.mlp.fc2.weight', 'dino.blocks.8.mlp.fc2.bias', 'dino.blocks.8.ls2.gamma', 'dino.blocks.9.norm1.weight', 'dino.blocks.9.norm1.bias', 'dino.blocks.9.attn.qkv.weight', 'dino.blocks.9.attn.qkv.bias', 'dino.blocks.9.attn.proj.weight', 'dino.blocks.9.attn.proj.bias', 'dino.blocks.9.ls1.gamma', 'dino.blocks.9.norm2.weight', 'dino.blocks.9.norm2.bias', 'dino.blocks.9.mlp.fc1.weight', 'dino.blocks.9.mlp.fc1.bias', 'dino.blocks.9.mlp.fc2.weight', 'dino.blocks.9.mlp.fc2.bias', 'dino.blocks.9.ls2.gamma', 'dino.blocks.10.norm1.weight', 'dino.blocks.10.norm1.bias', 'dino.blocks.10.attn.qkv.weight', 'dino.blocks.10.attn.qkv.bias', 'dino.blocks.10.attn.proj.weight', 'dino.blocks.10.attn.proj.bias', 'dino.blocks.10.ls1.gamma', 'dino.blocks.10.norm2.weight', 'dino.blocks.10.norm2.bias', 'dino.blocks.10.mlp.fc1.weight', 'dino.blocks.10.mlp.fc1.bias', 'dino.blocks.10.mlp.fc2.weight', 'dino.blocks.10.mlp.fc2.bias', 'dino.blocks.10.ls2.gamma', 'dino.blocks.11.norm1.weight', 'dino.blocks.11.norm1.bias', 'dino.blocks.11.attn.qkv.weight', 'dino.blocks.11.attn.qkv.bias', 'dino.blocks.11.attn.proj.weight', 'dino.blocks.11.attn.proj.bias', 'dino.blocks.11.ls1.gamma', 'dino.blocks.11.norm2.weight', 'dino.blocks.11.norm2.bias', 'dino.blocks.11.mlp.fc1.weight', 'dino.blocks.11.mlp.fc1.bias', 'dino.blocks.11.mlp.fc2.weight', 'dino.blocks.11.mlp.fc2.bias', 'dino.blocks.11.ls2.gamma', 'dino.norm.weight', 'dino.norm.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "model = DinoWithHead(AttentionReadout(head_dim=48, n_head=8)).to(device)\n",
    "head_state_dict = torch.load('attention_readout_8h_48dim_32out_5.pt', map_location=device)\n",
    "model.load_state_dict(head_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# embeddings, names = [], []\n",
    "\n",
    "# for batch_images, batch_names in tqdm(dataloader):\n",
    "#     batch_images = batch_images.to(device)\n",
    "#     with torch.inference_mode():\n",
    "#         out = model(batch_images)\n",
    "    \n",
    "#     embeddings.append(out.cpu())\n",
    "#     names.extend(batch_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = torch.cat(embeddings)\n",
    "\n",
    "# torch.save({'embeddings': embeddings, 'names': names}, 'embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load('embeddings.pt')\n",
    "embeddings = state['embeddings']\n",
    "names = state['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 64\n",
    "#rng = torch.Generator().manual_seed(42)\n",
    "#indices = torch.randperm(len(embeddings), generator=rng)[:n_examples]\n",
    "indices = torch.arange(n_examples)\n",
    "example_names = [names[i] for i in indices]\n",
    "#example_images = [dataset.load_by_name(name) for name in example_names]\n",
    "example_embeddings = embeddings[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def similarity(a: Tensor, b: Tensor) -> Tensor:\n",
    "    a_norm = nn.functional.normalize(a, dim=-1, p=2)\n",
    "    b_norm = nn.functional.normalize(b, dim=-1, p=2)\n",
    "    return torch.einsum(\"ic,jc->ij\", a_norm, b_norm)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class SimilarCoin:\n",
    "    file_name: str\n",
    "    similarity: float\n",
    "\n",
    "    def load_image(self) -> Tensor:\n",
    "        return dataset.load_by_name(self.file_name).permute(1, 2, 0)\n",
    "\n",
    "@dataclass\n",
    "class SimilarSeries:\n",
    "    example: str\n",
    "    similar_coins: list[SimilarCoin]\n",
    "\n",
    "    @classmethod\n",
    "    def from_values_and_indices(cls, example: str, values: Tensor, indices: Tensor) -> \"SimilarSeries\":\n",
    "        similar_coins = [SimilarCoin(names[i], v.item()) for i, v in zip(indices, values)]\n",
    "        return cls(example, similar_coins)\n",
    "\n",
    "    def plot(self):\n",
    "        n_similar = len(self.similar_coins)\n",
    "        nearest_square = int(n_similar ** 0.5)\n",
    "\n",
    "        fig, axes = plt.subplots(nearest_square, nearest_square, figsize=(20, 20), tight_layout=True)\n",
    "        for ax, coin in zip(axes.flat, self.similar_coins):\n",
    "            ax.imshow(coin.load_image())\n",
    "            coin_id = coin.file_name.removesuffix('.png')\n",
    "            ax.set_title(f\"{coin_id}\\n({coin.similarity:.2f})\")\n",
    "            ax.axis('off')\n",
    "        \n",
    "        return fig\n",
    "\n",
    "similarities = similarity(example_embeddings, embeddings)\n",
    "values, similar_indices = similarities.topk(25, dim=-1)\n",
    "\n",
    "series = []\n",
    "for example, value, indices in zip(example_names, values, similar_indices):\n",
    "    series.append(SimilarSeries.from_values_and_indices(example, value, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 25])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in series:\n",
    "    fig = s.plot()\n",
    "    fig.savefig(f'similarity_tables/{s.example}')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
